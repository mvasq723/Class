{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Max_Vasquez_Spark_Prophet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXXqPQT/89HYAqEEfNM+44",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvasq723/Class/blob/main/Max_Vasquez_Spark_Prophet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjFsN9fWZaQe",
        "outputId": "d47f2f9a-d4d1-4306-919e-b901c2e6c13e"
      },
      "source": [
        "# Running Spark in Colab For Time Series\r\n",
        "# BASED ON CODE FROM Anik (2019) AND Thomas (2020)\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "!apt update\r\n",
        "\r\n",
        "import timeit\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,360 kB]\n",
            "Fetched 2,612 kB in 3s (990 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "33 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8maRTqN2Z2Lx"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\r\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtJQ5296aA77"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTrZfJfOaIHO"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hFfYlywaY1c"
      },
      "source": [
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\r\n",
        "from pyspark.sql.types import StructType,StructField,StringType,LongType,DoubleType,IntegerType,FloatType,DateType,TimestampType\r\n",
        "\r\n",
        "import statsmodels.tsa.api as sm\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from fbprophet import Prophet\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnUA4inRdOGl"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\r\n",
        "from pyspark.ml.regression import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOdQ-PRyaV8q",
        "outputId": "a51d468e-ff7f-4c00-f12d-0464dcedf34e"
      },
      "source": [
        "# Load Data to realestate\r\n",
        "realestate = pd.read_csv(\"http://files.zillowstatic.com/research/public/Zip/Zip_Zhvi_SingleFamilyResidence.csv\")\r\n",
        "print(realestate)\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       RegionID  SizeRank  RegionName RegionType StateName  ... 2019-11-30  \\\n",
            "0         61639         0       10025        Zip        NY  ...  1375725.0   \n",
            "1         84654         1       60657        Zip        IL  ...   973104.0   \n",
            "2         61637         2       10023        Zip        NY  ...  1480426.0   \n",
            "3         91982         3       77494        Zip        TX  ...   336083.0   \n",
            "4         84616         4       60614        Zip        IL  ...  1201182.0   \n",
            "...         ...       ...         ...        ...       ...  ...        ...   \n",
            "30459     58111     35187         802        Zip        UT  ...   130717.0   \n",
            "30460     58115     35187         820        Zip        LA  ...   101363.0   \n",
            "30461     58117     35187         822        Zip        LA  ...   175176.0   \n",
            "30462     58121     35187         831        Zip        AL  ...    74273.0   \n",
            "30463     58125     35187         851        Zip        CO  ...   450047.0   \n",
            "\n",
            "      2019-12-31 2020-01-31 2020-02-29  2020-03-31  \n",
            "0      1374714.0  1381453.0  1385737.0   1389268.0  \n",
            "1       971908.0   972038.0   973671.0    975642.0  \n",
            "2      1476509.0  1478980.0  1479301.0   1474994.0  \n",
            "3       336154.0   335860.0   336037.0    336483.0  \n",
            "4      1198879.0  1198277.0  1199900.0   1200980.0  \n",
            "...          ...        ...        ...         ...  \n",
            "30459   130725.0   130869.0   131717.0    132127.0  \n",
            "30460   100458.0   100233.0   100404.0    100708.0  \n",
            "30461   175615.0   176689.0   178685.0    181195.0  \n",
            "30462    74583.0    74891.0    75145.0     75464.0  \n",
            "30463   450546.0   452646.0   454544.0    456250.0  \n",
            "\n",
            "[30464 rows x 300 columns]\n",
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG73ZL1cawIh",
        "outputId": "f6f00729-b743-4703-dcc0-cdff3e498e9d"
      },
      "source": [
        "pd.set_option('display.max_rows', 10)\r\n",
        "pd.set_option('display.max_columns', 10)\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pzw2oUHa2Le",
        "outputId": "6b16bf86-4653-4f2c-cd69-a440511866de"
      },
      "source": [
        "# Change realestate data to dataframe\r\n",
        "realestate=pd.DataFrame(realestate)\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1afYfBGa5LF",
        "outputId": "cdc46733-e53c-48bd-f6b2-a353fb6ea4fb"
      },
      "source": [
        "##################################\r\n",
        "#Data Munging (Original)\r\n",
        "##################################\r\n",
        "# Identify length of rows and columns\r\n",
        "norows=len(realestate.RegionID)\r\n",
        "nocol=len(realestate.columns)-9                                               # -9 because the 1st 9 columns not being transposed\r\n",
        "\r\n",
        "# Create new data frame with columns not being being transposed\r\n",
        "realestatelong=realestate[['RegionID','RegionName','City','State','Metro','CountyName','SizeRank']].astype(str)\r\n",
        "realestatelong=pd.concat([realestatelong]*nocol, ignore_index=True)\r\n",
        "\r\n",
        "# Add new columns\r\n",
        "realestatelong['date']=''\r\n",
        "realestatelong['value']=''\r\n",
        "\r\n",
        "#first row\r\n",
        "x=0\r\n",
        "#last row\r\n",
        "y=30464\r\n",
        "#Starting column\r\n",
        "z=9\r\n",
        "\r\n",
        "# Loop\r\n",
        "#--------------------------\r\n",
        "for i in range(nocol):\r\n",
        "\r\n",
        "  realestatelong.date[x:y]=realestate.columns[z]  \r\n",
        "  realestatelong.value[x:y]=realestate.iloc[:,z]\r\n",
        " \r\n",
        "  x=x+norows\r\n",
        "  y=y+norows\r\n",
        "  z=z+1\r\n",
        "\r\n",
        "# end of loop ----------------------------\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2cHMmCOa-Vr",
        "outputId": "9975d39c-e2e1-480b-c020-5d9b6122bc87"
      },
      "source": [
        "#sort data by Region Name and date\r\n",
        "realestatelong=realestatelong.sort_values([\"RegionName\",\"date\"], ascending=(True,True))\r\n",
        "print(realestatelong)\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        RegionID RegionName      City State                        Metro  \\\n",
            "1579       61615      10001  New York    NY  New York-Newark-Jersey City   \n",
            "32043      61615      10001  New York    NY  New York-Newark-Jersey City   \n",
            "62507      61615      10001  New York    NY  New York-Newark-Jersey City   \n",
            "92971      61615      10001  New York    NY  New York-Newark-Jersey City   \n",
            "123435     61615      10001  New York    NY  New York-Newark-Jersey City   \n",
            "...          ...        ...       ...   ...                          ...   \n",
            "8729552   100489      99929  Wrangell    AK                          nan   \n",
            "8760016   100489      99929  Wrangell    AK                          nan   \n",
            "8790480   100489      99929  Wrangell    AK                          nan   \n",
            "8820944   100489      99929  Wrangell    AK                          nan   \n",
            "8851408   100489      99929  Wrangell    AK                          nan   \n",
            "\n",
            "               CountyName SizeRank        date   value  \n",
            "1579      New York County     1588  1996-01-31  279614  \n",
            "32043     New York County     1588  1996-02-29  284673  \n",
            "62507     New York County     1588  1996-03-31  285884  \n",
            "92971     New York County     1588  1996-04-30  292432  \n",
            "123435    New York County     1588  1996-05-31  295610  \n",
            "...                   ...      ...         ...     ...  \n",
            "8729552  Wrangell Borough    17211  2019-11-30  223293  \n",
            "8760016  Wrangell Borough    17211  2019-12-31  224709  \n",
            "8790480  Wrangell Borough    17211  2020-01-31  226174  \n",
            "8820944  Wrangell Borough    17211  2020-02-29  227580  \n",
            "8851408  Wrangell Borough    17211  2020-03-31  229120  \n",
            "\n",
            "[8865024 rows x 9 columns]\n",
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj0tFtsxcruf",
        "outputId": "971a19d3-08c4-42ec-8c74-a419c39bc5fc"
      },
      "source": [
        "#Drop Unwanted columns\r\n",
        "re_prophet=realestatelong.drop(columns = ['RegionID', 'State', 'Metro', 'CountyName', 'SizeRank']) #drop unenecessary columns\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8bQi2n6cLLH",
        "outputId": "6b0995d4-1fb4-495f-a874-17c515993dbc"
      },
      "source": [
        "#Change date column from object to datetime\r\n",
        "re_prophet.date=pd.to_datetime(re_prophet['date'], format='%Y-%m-%d')\r\n",
        "\r\n",
        "#Change value from object to float\r\n",
        "re_prophet.value=re_prophet.value.astype(float)\r\n",
        "\r\n",
        "#Check for Nan\r\n",
        "print(\"\\nFeatures and data types : \\n\",re_prophet.dtypes)\r\n",
        "print(\"\\nMissing values : \\n\" , re_prophet.isnull().any())\r\n",
        "print(\"\\nUnique values : \\n\" , re_prophet.nunique())\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Features and data types : \n",
            " RegionName            object\n",
            "City                  object\n",
            "date          datetime64[ns]\n",
            "value                float64\n",
            "dtype: object\n",
            "\n",
            "Missing values : \n",
            " RegionName    False\n",
            "City          False\n",
            "date          False\n",
            "value          True\n",
            "dtype: bool\n",
            "\n",
            "Unique values : \n",
            " RegionName     30464\n",
            "City           14862\n",
            "date             291\n",
            "value         727935\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFz60Cnyi2wO",
        "outputId": "70ef3106-d414-4cd3-9534-4e1231803eea"
      },
      "source": [
        "##########partition test\r\n",
        "#re_prophet\r\n",
        "#x=30464/7\r\n",
        "#print(x)\r\n",
        "#y=x*291\r\n",
        "#print(y)\r\n",
        "#df.iloc[:1000,:]\r\n",
        "#print(re_prophet.iloc[:1266432,:] ) \r\n",
        "#print(re_prophet.iloc[1266432:2532864,:] )\r\n",
        "#print(re_prophet.iloc[2532864:3799296,:] )\r\n",
        "#print(re_prophet.iloc[3799296:5065728,:] )\r\n",
        "#print(re_prophet.iloc[5065728:6332160,:] )\r\n",
        "#print(re_prophet.iloc[6332160:7598592,:] )\r\n",
        "#print(re_prophet.iloc[7598592:,:] )\r\n",
        "\r\n",
        "\r\n",
        "re_prophet0=re_prophet.iloc[:291,:]\r\n",
        "re_prophet1=re_prophet.iloc[:1266432,:]\r\n",
        "re_prophet2=re_prophet.iloc[1266432:2532864,:]\r\n",
        "re_prophet3=re_prophet.iloc[2532864:3799296,:]\r\n",
        "re_prophet4=re_prophet.iloc[3799296:5065728,:]\r\n",
        "re_prophet5=re_prophet.iloc[5065728:6332160,:]\r\n",
        "re_prophet6=re_prophet.iloc[6332160:7598592,:]\r\n",
        "re_prophet7=re_prophet.iloc[7598592:,:]\r\n",
        "print(re_prophet0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        RegionName      City       date      value\n",
            "1579         10001  New York 1996-01-31   279614.0\n",
            "32043        10001  New York 1996-02-29   284673.0\n",
            "62507        10001  New York 1996-03-31   285884.0\n",
            "92971        10001  New York 1996-04-30   292432.0\n",
            "123435       10001  New York 1996-05-31   295610.0\n",
            "...            ...       ...        ...        ...\n",
            "8714283      10001  New York 2019-11-30  1160163.0\n",
            "8744747      10001  New York 2019-12-31  1153438.0\n",
            "8775211      10001  New York 2020-01-31  1154711.0\n",
            "8805675      10001  New York 2020-02-29  1151760.0\n",
            "8836139      10001  New York 2020-03-31  1149756.0\n",
            "\n",
            "[291 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBfsXgrVcZDD"
      },
      "source": [
        "#re_prophet.to_csv('re_prophet.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBSBp_7sdYzo",
        "outputId": "d9f052c6-9d6c-4d5a-fb09-b55525d75553"
      },
      "source": [
        "schema = StructType([StructField('RegionName', StringType(), True),\r\n",
        "                     StructField('City', StringType(), True),\r\n",
        "                     StructField('date', TimestampType(), True),\r\n",
        "                     StructField('value', DoubleType(), True)])\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDIggOrNdAgR",
        "outputId": "93f98268-8b56-4b0b-c53f-cf208885f567"
      },
      "source": [
        "#create a spark dataframe\r\n",
        "sre_prophet=spark.createDataFrame(re_prophet0,schema)\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FanXCsEcd9SI",
        "outputId": "2e176feb-8a7f-4708-968e-1f267f991a9d"
      },
      "source": [
        "sre_prophet.show(5)\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------+-------------------+--------+\n",
            "|RegionName|    City|               date|   value|\n",
            "+----------+--------+-------------------+--------+\n",
            "|     10001|New York|1996-01-31 00:00:00|279614.0|\n",
            "|     10001|New York|1996-02-29 00:00:00|284673.0|\n",
            "|     10001|New York|1996-03-31 00:00:00|285884.0|\n",
            "|     10001|New York|1996-04-30 00:00:00|292432.0|\n",
            "|     10001|New York|1996-05-31 00:00:00|295610.0|\n",
            "+----------+--------+-------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYkKNKySeABL",
        "outputId": "4f1740c0-4e28-4b56-d4b7-f84a58b0b0e2"
      },
      "source": [
        "sre_prophet.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- RegionName: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- value: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92kb1qqQeG8E",
        "outputId": "04243cb0-1065-4253-8df3-418f4b1804b0"
      },
      "source": [
        "sre_prophet.select(['RegionName']).groupBy('RegionName').agg({'RegionName':'count'}).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------------+\n",
            "|RegionName|count(RegionName)|\n",
            "+----------+-----------------+\n",
            "|     10001|              291|\n",
            "+----------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYbOSlcbeWho"
      },
      "source": [
        "sre_prophet.createOrReplaceTempView(\"value\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQk8L85feYoC",
        "outputId": "8fef09fd-2ec2-45a4-c402-612422b0268c"
      },
      "source": [
        "spark.sql(\"select RegionName, count(*) from value group by RegionName order by RegionName\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------+\n",
            "|RegionName|count(1)|\n",
            "+----------+--------+\n",
            "|     10001|     291|\n",
            "+----------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40SDVjKGekOy"
      },
      "source": [
        "sql = 'SELECT RegionName, date as ds, sum(value) as y FROM value GROUP BY RegionName, ds ORDER BY RegionName, ds'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AmsDJA3enJX",
        "outputId": "d2a135a4-1613-4e44-ac78-e714753743b7"
      },
      "source": [
        "spark.sql(sql).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------------+--------+\n",
            "|RegionName|                 ds|       y|\n",
            "+----------+-------------------+--------+\n",
            "|     10001|1996-01-31 00:00:00|279614.0|\n",
            "|     10001|1996-02-29 00:00:00|284673.0|\n",
            "|     10001|1996-03-31 00:00:00|285884.0|\n",
            "|     10001|1996-04-30 00:00:00|292432.0|\n",
            "|     10001|1996-05-31 00:00:00|295610.0|\n",
            "|     10001|1996-06-30 00:00:00|301629.0|\n",
            "|     10001|1996-07-31 00:00:00|305750.0|\n",
            "|     10001|1996-08-31 00:00:00|311539.0|\n",
            "|     10001|1996-09-30 00:00:00|319590.0|\n",
            "|     10001|1996-10-31 00:00:00|325514.0|\n",
            "|     10001|1996-11-30 00:00:00|330398.0|\n",
            "|     10001|1996-12-31 00:00:00|333069.0|\n",
            "|     10001|1997-01-31 00:00:00|338388.0|\n",
            "|     10001|1997-02-28 00:00:00|344245.0|\n",
            "|     10001|1997-03-31 00:00:00|352668.0|\n",
            "|     10001|1997-04-30 00:00:00|356969.0|\n",
            "|     10001|1997-05-31 00:00:00|364187.0|\n",
            "|     10001|1997-06-30 00:00:00|366519.0|\n",
            "|     10001|1997-07-31 00:00:00|371849.0|\n",
            "|     10001|1997-08-31 00:00:00|375867.0|\n",
            "+----------+-------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcACqK2KfFX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650c4723-dfe2-4863-d2c4-a80ac92fd06b"
      },
      "source": [
        "sre_prophet.explain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "Scan ExistingRDD[RegionName#3176,City#3177,date#3178,value#3179]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09dJ3uo1gy4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420f9781-bc13-4d84-fdf6-766f1fb31702"
      },
      "source": [
        "sre_prophet.rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbeZuvNTg1H5"
      },
      "source": [
        "RegionName_part = (spark.sql(sql).repartition(spark.sparkContext.defaultParallelism, ['RegionName'])).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvxms2fXg4Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26526d06-fc33-4a0e-82ac-56c609fb7599"
      },
      "source": [
        "RegionName_part"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[RegionName: string, ds: timestamp, y: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjvooxppg7QG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2f87e9-1afc-4d8e-8898-5750c8d35ebd"
      },
      "source": [
        "RegionName_part.explain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.explain of DataFrame[RegionName: string, ds: timestamp, y: double]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boEZPJjZg-Da"
      },
      "source": [
        "#Create result schema\r\n",
        "result_schema = StructType({\r\n",
        "    StructField('ds',TimestampType()),\r\n",
        "    StructField('RegionName',StringType()),\r\n",
        "    StructField('y',DoubleType()),\r\n",
        "    StructField('yhat',DoubleType()),\r\n",
        "    StructField('yhat_upper',DoubleType()),\r\n",
        "    StructField('yhat_lower',DoubleType()),\r\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aXbDnPbg_De",
        "outputId": "c7e68b5c-9efb-4335-d6a0-2d8fe4152737"
      },
      "source": [
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\r\n",
        "\r\n",
        "@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )\r\n",
        "def forecast_values( RegionName_pd ):\r\n",
        "  model = Prophet(interval_width=0.95)\r\n",
        "  model.fit( RegionName_pd )\r\n",
        "  future_pd = model.make_future_dataframe(\r\n",
        "      periods=5,\r\n",
        "      freq='w'\r\n",
        "      )\r\n",
        "  forecast_pd = model.predict ( future_pd )\r\n",
        "\r\n",
        "  f_pd = forecast_pd[ ['ds', 'yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\r\n",
        "\r\n",
        "  st_pd = RegionName_pd[['ds', 'RegionName', 'y']].set_index('ds')\r\n",
        "\r\n",
        "  results_pd = f_pd.join( st_pd, how = 'left')\r\n",
        "  results_pd.reset_index(level=0, inplace=True)\r\n",
        "\r\n",
        "  results_pd['RegionName']=  RegionName_pd['RegionName'].iloc[0]\r\n",
        "\r\n",
        "  return results_pd[ ['ds', 'RegionName', 'y', 'yhat','yhat_upper', 'yhat_lower'] ]\r\n",
        "\r\n",
        "print(\"\\n\\n\\n-----Code executed succesfully-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "-----Code executed succesfully-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bhzzvi2hVoO"
      },
      "source": [
        "results = (\r\n",
        "    RegionName_part \r\n",
        "      .groupBy('RegionName')\r\n",
        "      .apply(forecast_values)\r\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMzCXp0ZhYaw",
        "outputId": "d743cae8-d155-49b0-939b-cc92d77744e7"
      },
      "source": [
        "results.cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[ds: timestamp, yhat: double, RegionName: string, yhat_upper: double, y: double, yhat_lower: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuUUUVE3hlrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2e5c05-13e2-44ab-fdac-8a9651efd331"
      },
      "source": [
        "results.explain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "InMemoryTableScan [ds#3281, yhat#3282, RegionName#3283, yhat_upper#3284, y#3285, yhat_lower#3286]\n",
            "   +- InMemoryRelation [ds#3281, yhat#3282, RegionName#3283, yhat_upper#3284, y#3285, yhat_lower#3286], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "         +- FlatMapGroupsInPandas [RegionName#3176], forecast_values(RegionName#3176, ds#3250, y#3251), [ds#3281, yhat#3282, RegionName#3283, yhat_upper#3284, y#3285, yhat_lower#3286]\n",
            "            +- *(1) Sort [RegionName#3176 ASC NULLS FIRST], false, 0\n",
            "               +- InMemoryTableScan [RegionName#3176, RegionName#3176, ds#3250, y#3251]\n",
            "                     +- InMemoryRelation [RegionName#3176, ds#3250, y#3251], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                           +- Exchange hashpartitioning(RegionName#3176, 2)\n",
            "                              +- *(3) Sort [RegionName#3176 ASC NULLS FIRST, ds#3250 ASC NULLS FIRST], true, 0\n",
            "                                 +- Exchange rangepartitioning(RegionName#3176 ASC NULLS FIRST, ds#3250 ASC NULLS FIRST, 200)\n",
            "                                    +- *(2) HashAggregate(keys=[RegionName#3176, date#3178], functions=[sum(value#3179)])\n",
            "                                       +- Exchange hashpartitioning(RegionName#3176, date#3178, 200)\n",
            "                                          +- *(1) HashAggregate(keys=[RegionName#3176, date#3178], functions=[partial_sum(value#3179)])\n",
            "                                             +- *(1) Project [RegionName#3176, date#3178, value#3179]\n",
            "                                                +- Scan ExistingRDD[RegionName#3176,City#3177,date#3178,value#3179]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkiOBnYXhn0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e5d3c9-32ba-4beb-dd2d-0670839e51ee"
      },
      "source": [
        "results.coalesce"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.coalesce of DataFrame[ds: timestamp, yhat: double, RegionName: string, yhat_upper: double, y: double, yhat_lower: double]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB1dVtPQhqlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6596457-ec8a-45eb-8c37-820119c24e92"
      },
      "source": [
        "results.count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.count of DataFrame[ds: timestamp, yhat: double, RegionName: string, yhat_upper: double, y: double, yhat_lower: double]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RpL3MhRShrVk",
        "outputId": "fdb0bef9-9710-45a3-d97b-45dba4498770"
      },
      "source": [
        "results.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-314-8c647e8bf4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1780.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 199.0 failed 1 times, most recent failure: Lost task 0.0 in stage 199.0 (TID 7688, localhost, executor driver): java.lang.UnsupportedOperationException\n\tat org.apache.spark.sql.vectorized.ArrowColumnVector$ArrowVectorAccessor.getLong(ArrowColumnVector.java:215)\n\tat org.apache.spark.sql.vectorized.ArrowColumnVector.getLong(ArrowColumnVector.java:89)\n\tat org.apache.spark.sql.execution.vectorized.MutableColumnarRow.getLong(MutableColumnarRow.java:120)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:94)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:84)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.UnsupportedOperationException\n\tat org.apache.spark.sql.vectorized.ArrowColumnVector$ArrowVectorAccessor.getLong(ArrowColumnVector.java:215)\n\tat org.apache.spark.sql.vectorized.ArrowColumnVector.getLong(ArrowColumnVector.java:89)\n\tat org.apache.spark.sql.execution.vectorized.MutableColumnarRow.getLong(MutableColumnarRow.java:120)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:94)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:84)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    }
  ]
}